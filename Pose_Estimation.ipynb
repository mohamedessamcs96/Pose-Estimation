{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHgtagwNPAKo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_lightning_3.tflite')\n",
        "interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "sMcE120eVAqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "\n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 4, (0,255,0), -1)"
      ],
      "metadata": {
        "id": "RmBCfg8bVP6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Nose\n",
        "1. Left Eye\n",
        "2. Right Eye\n",
        "3. Left Ear\n",
        "4. Right Ear\n",
        "5. Left Shoulder\n",
        "6. Right Shoulder\n",
        "7. Left Elbow\n",
        "8. Right Elbow\n",
        "9. Left Wrist\n",
        "10. Right Wrist\n",
        "11. Left Hip\n",
        "12. Right Hip\n",
        "13. Left Knee\n",
        "14. **Right Knee**\n",
        "15. Left Ankle\n",
        "16. Right Ankle"
      ],
      "metadata": {
        "id": "4nvtCEFNbF4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}"
      ],
      "metadata": {
        "id": "r-3z5CybVchy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "\n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "\n",
        "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):\n",
        "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
      ],
      "metadata": {
        "id": "GijXPju5VewJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture('vtest.avi')  # Replace 'video_filename.mp4' with your video file's path\n",
        "\n",
        "# Check if the video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "else:\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Reshape image\n",
        "        img = frame.copy()\n",
        "        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
        "        input_image = tf.cast(img, dtype=tf.float32)\n",
        "\n",
        "        # Setup input and output\n",
        "        input_details = interpreter.get_input_details()\n",
        "        output_details = interpreter.get_output_details()\n",
        "\n",
        "        # Make predictions\n",
        "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "        interpreter.invoke()\n",
        "        keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # Rendering\n",
        "        draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
        "        draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
        "        from IPython.display import display, clear_output\n",
        "        #Display the frame in the Colab notebook using IPython.display\n",
        "        display_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        #display(display_frame)\n",
        "\n",
        "        # Clear the previous frame for a smooth video display\n",
        "        clear_output(wait=True)\n",
        "        # Display the frame using matplotlib\n",
        "        plt.imshow(display_frame)\n",
        "        plt.axis('off')  # Turn off axis labels\n",
        "        plt.show()\n",
        "\n",
        "        # Find the indices of the right and left knee keypoints (if available)\n",
        "        right_knee_index = None\n",
        "        left_knee_index = None\n",
        "\n",
        "        for idx, keypoint in enumerate(keypoints_with_scores[0]):\n",
        "               x, y = keypoint[:2]  # Extract X and Y coordinates\n",
        "               # Check if the coordinates are valid (non-negative values)\n",
        "               if (x >= 0).all() and (y >= 0).all():\n",
        "\n",
        "                  # Check the position of the keypoint to determine if it corresponds to the right or left knee\n",
        "                  # You may need to adjust the position criteria based on your model's output\n",
        "                  if (x > 90).all():  # Adjust the threshold as needed\n",
        "                      right_knee_index = idx\n",
        "                  else:\n",
        "                      left_knee_index = idx\n",
        "\n",
        "        # Print the right knee keypoint if it was found\n",
        "        if right_knee_index is not None:\n",
        "            right_knee_keypoint = keypoints_with_scores[0][right_knee_index]\n",
        "            right_knee_x = right_knee_keypoint[1]\n",
        "            right_knee_y = right_knee_keypoint[0]\n",
        "            confidence = right_knee_keypoint[2]\n",
        "            print(f\"Right Knee Coordinates - X: {right_knee_x}, Y: {right_knee_y}\")\n",
        "            print(f\"Confidence: {confidence}\")\n",
        "        else:\n",
        "            print(\"Right knee not detected in this frame\")\n",
        "\n",
        "        # Print the left knee keypoint if it was found\n",
        "        if left_knee_index is not None:\n",
        "            left_knee_keypoint = keypoints_with_scores[0][left_knee_index]\n",
        "            left_knee_x = left_knee_keypoint[1]\n",
        "            left_knee_y = left_knee_keypoint[0]\n",
        "            confidence = left_knee_keypoint[2]\n",
        "            print(f\"Left Knee Coordinates - X: {left_knee_x}, Y: {left_knee_y}\")\n",
        "            print(f\"Confidence: {confidence}\")\n",
        "        else:\n",
        "            print(\"Left knee not detected in this frame\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Press 'q' to exit the video playback\n",
        "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "MDFYSv1GV-6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Reshape image\n",
        "    img = frame.copy()\n",
        "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
        "    input_image = tf.cast(img, dtype=tf.float32)\n",
        "\n",
        "    # Setup input and output\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Make predictions\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "    interpreter.invoke()\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Rendering\n",
        "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
        "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
        "\n",
        "    cv2.imshow('MoveNet Lightning', frame)\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "1S9W27tImtEN"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}